---
title: Analyze Transit Patterns with Apache NiFi
author: sandbox-team
tutorial-id: 640
experience: Intermediate
persona: Data Scientist & Analyst
source: Hortonworks
use case: Flow Management
technology: Apache NiFi
release: hdf-3.2.0
environment: Sandbox
product: HDF
series: HDF > Develop Data Flow & Streaming Applications > Real World Examples
---

# 1.Analyze Transit Patterns with Apache NiFi

## Introduction

Apache NiFi is the first integrated platform that solves the real-time challenges of collecting and transporting data from a multitude of sources and provides interactive command and control of live flows with full and automated data provenance. NiFi provides the data acquisition, simple event processing, transport and delivery mechanism designed to accommodate the diverse dataflows generated by a world of connected people, systems, and things.

For the purposes of this tutorial, assume that a city planning board is evaluating the need for a new highway.  This decision is dependent on current traffic patterns, particularly as other roadwork initiatives are under way. Integrating live data poses a problem because traffic analysis has traditionally been done using historical, aggregated traffic counts.  To improve traffic analysis, the city planner wants to leverage real-time data to get a deeper understanding of traffic patterns.  NiFi was selected for this real-time data integration.

## Goals and Objectives

The goal of this tutorial is to provide you with an opportunity to interact with Apache NiFi features while building a dataflow. You do not need programming experience or flow-based programming syntax and feature knowledge to successfully complete this tutorial.

The learning objectives of this tutorial are to:

- Understand Apache NiFi fundamentals
- Introduce NiFi’s HTML user interface
- Introduce NiFi processor configuration, relationships, data provenance, and documentation
- Create dataflows
- Incorporate APIs into a NiFi dataflow
- Learn about NiFi templates
- Create Process Groups

## Prerequisites

- Downloaded and deployed the [Hortonworks DataFlow (HDF)](https://www.cloudera.com/downloads/hortonworks-sandbox/hdf.html?utm_source=mktg-tutorial) Sandbox
- For Windows 10 users, use [Ubuntu bash shell](https://www.windowscentral.com/how-install-bash-shell-command-line-windows-10) or Sandbox Web Shell Client

## Outline

In this tutorial, we work with San Francisco MUNI Transit agency data, gathered from NextBus XML Live Feed, handling vehicle locations, speeds, and other variables.

The tutorial consists of seven sections:

1. [NiFi DataFlow Automation Concepts](https://hortonworks.com/tutorial/analyze-transit-patterns-with-apache-nifi/section/1/) - Explore the fundamentals of Data Flow Management with NiFi: Core Concepts, Architecture, etc
2. [Launch NiFi HTML UI](https://hortonworks.com/tutorial/analyze-transit-patterns-with-apache-nifi/section/2/) - Launch your NiFi HTML User Interface (UI). Get NiFi up and running on the HDF Sandbox.
3. [Build a NiFi Process Group to Simulate NextBus API](https://hortonworks.com/tutorial/analyze-transit-patterns-with-apache-nifi/section/3/) - Simulate the NextBus API live feed with a data seed and check the data generating from the simulator.
4. [Build a NiFi Process Group to Parse Transit Events](https://hortonworks.com/tutorial/analyze-transit-patterns-with-apache-nifi/section/4/) - Parse the XML file for transit observations(vehicle location, speed, vehicle ID, etc).
5. [Build a NiFi Process Group to Validate the GeoEnriched Data](https://hortonworks.com/tutorial/analyze-transit-patterns-with-apache-nifi/section/5/) - Integrate Google Places API to bring more meaningful geographic insights and validate them.
6. [Build a NiFi Process Group to Store Data As JSON](https://hortonworks.com/tutorial/analyze-transit-patterns-with-apache-nifi/section/6/) - Convert XML to JSON data format and store into file on local file system.
7. [Ingest Live Vehicle Routes via NextBus API](https://hortonworks.com/tutorial/analyze-transit-patterns-with-apache-nifi/section/7/) - Ingest NextBus's live stream data for San Francisco MUNI agency.

Each tutorial provides step by step instructions, so that you can complete the learning objectives and tasks associated with it. You are also provided with a dataflow template for each tutorial that you can use for verification. Each tutorial builds on the previous.


---
title: NiFi DataFlow Automation Concepts
---

# 1.NiFi DataFlow Automation Concepts

## Introduction

The concepts section is tailored toward enriching your hands-on experience in the tutorials. By the end of this section, you will be able to define NiFi, know how to create dataflows for specific use cases, acquire knowledge on how to build a NiFi DataFlow and become familiar with the core concepts of NiFi. The goal of this section is to help NiFi practitioners know how to use the NiFi documentation for their advantage.

## Outline

- [What is Apache NiFi?](#what-is-apache-nifi)
- [Who Uses NiFi, and for What?](#who-uses-nifi-and-for-what)
- [Understand NiFi DataFlow Build Process](#understand-nifi-dataflow-build-process)
- [The Core Concepts of NiFi](#the-core-concepts-of-nifi)
- [NiFi Architecture](#nifi-architecture)
- [A Brief History of NiFi](#a-brief-history-of-nifi)
- [Further Reading](#further-reading)

## What is Apache NiFi?

[Apache NiFi](https://nifi.apache.org/docs/nifi-docs/html/overview.html#what-is-apache-nifi) is an open source tool for automating and managing the flow of data between systems (Databases, Sensors, Data Lakes, Data Platforms). In the tutorial, we will use NiFi to process the flow of data between sensors, web services (NextBus and Google Places API), various locations and our local file system.

## Who Uses NiFi, and for What?

NiFi is used for **data ingestion** to pull data into NiFi, from numerous different data sources and create FlowFiles. For the tutorial, GetFile, GetHTTP, InvokeHTTP are processors you will use to stream data into NiFi from the local file system and ingest data from the internet. Once the data is ingested, as the DataFlow Manager (DFM), the user, you will create 2 process groups or sections of the dataflow that handle a particular purpose in data preprocessing. A process group is a complex processor composed of multiple processors. You will create a **Data Management** process group to monitor and obtain feedback about the current status of the NiFi DataFlow. You will use bulletins to troubleshoot problems in the dataflow. Bulletins are located on the processor and the management toolbar. They provide a tool-tip of the time, severity and message of the alert. While the data is managed, you will create **Data Enrichment** process group to enhance, refine and improve the quality of data to make it meaningful and valuable for users. NiFi enables users to filter out unnecessary information from data to make easier to understand. You will use NiFi to geographically enrich real-time data to show neighborhoods nearby locations as the locations change.

## Understand NiFi DataFlow Build Process

### Explore NiFi HTML User Interface

When NiFi is accessed at `sandbox-hdf.hortonworks.com:9090/nifi` by users who run NiFi from Hortonworks DataFlow (HDF) sandbox or `sandbox.hortonworks.com:8080/nifi` by users who run NiFi from their local machine, [NiFi's User Interface (UI)](https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.3/bk_user-guide/content/User_Interface.html) appears on the screen. The UI is where dataflows will be developed. It includes a canvas and _mechanisms_ to build, visualize, monitor, edit, and administer our dataflows in the tutorials. The **components** toolbar contains all tools for building the dataflow. The **actions** toolbar consists of buttons that manipulate the components on the canvas. The **management** toolbar has buttons for the DFM to manage the flow and a NiFi administrator to manage user access & system properties. The **search** toolbar enables users to search for any component in the dataflow. The image below shows a visualization of where each mechanism is located.

![nifi_dataflow_html_interface](assets/tutorial-1-nifi-dataflow-automation-concepts/nifi_dataflow_html_interface.png)

### Find and Add Processor Overview

Every dataflow requires a set of processors. In the tutorials, you will use the processor icon ![processor_nifi_iot](assets/tutorial-1-nifi-dataflow-automation-concepts/processor_nifi.png) to add processors to the dataflow. Let’s view the add processor window. There are 3 options to find our desired processor. The **processor list** contains almost 190 items with descriptions for each processor. The **tag cloud** reduces the list by category, so if you know what particular use case your desired processor is associated with, select the tag and find the appropriate processor faster. The **filter bar** searches for the processor based on the keyword entered. The image below illustrates where each option is located on the add processor window.

![add_processor_window](assets/tutorial-1-nifi-dataflow-automation-concepts/add_processor_window.jpg)

### Configure Processor Dialog

As we add each processor to our dataflow, we must make sure they are properly configured. DataFlow Managers navigate around the 4 configuration tabs to control the processor's specific behavior and instruct the processor on how to process the data that is flowing. Let's explore these tabs briefly. The **Settings** tab allows users to change the processor's name, define relationships & includes many different parameters. The **Scheduling** tab affects how the processor is scheduled to run. The **Properties** tab affects the processor's specific behavior. The **Comments** tab provides a place for DFMs to include useful information about the processor's use-case. For the tutorial series, you will spend most of time modifying properties.

![putfile_logs_nifi_iot](assets/tutorial-1-nifi-dataflow-automation-concepts/putfile_logs_nifi_iot.png)

### Configure Processor Properties Tab

Let's further explore the properties tab, so we can be familiar with this tab in advance for the tutorials. If you want to know more about what a particular property does, hover over the **help symbol** ![question_mark_symbol_properties_config_iot](assets/tutorial-1-nifi-dataflow-automation-concepts/question_mark_symbol_properties_config_iot.png) located next to the property name to find additional details about that property, its value and history. Some processors enable the DFM to add new properties into the property table. For the tutorials, you will add user-defined properties into processors, such as **UpdateAttribute**. The custom user-defined property you create will assign unique filenames to each FlowFile that transfer through this processor. View the processor properties tab below:

![updateAttribute_config_property_tab_window](assets/tutorial-1-nifi-dataflow-automation-concepts/updateAttribute_config_property_tab_window.png)

### Connections & relationships

As each processor configuration is completed, we must [connect](https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.3/bk_user-guide/content/Connecting_Components.html) it to another component. A connection is a linkage between processors (or components) that contain at least one relationship. The user selects the relationship and based on the processing outcome that will determine where the data is routed. Processors can have zero or more auto-terminate relationships. If the processing outcome for FlowFile is true for a processor with a relationship tied to itself, the FlowFile will be removed from the flow. For instance, if **EvaluateXPath** has an unmatched relationship defined to itself and when that outcome is true, then a FlowFile is removed from the flow. Else a FlowFile is routed to the next processor based on matched. View the visual to see the objects that define connections and relationships.

![completed-data-flow-lab1-connection_relationship_concepts](assets/tutorial-1-nifi-dataflow-automation-concepts/connection_relationship_concepts.png)

### Troubleshoot Common Processor Issues

When a processor has a warning symbol ![warning_symbol_nifi_iot](assets/tutorial-1-nifi-dataflow-automation-concepts/warning_symbol_nifi_iot.png) in the upper left corner of the processor face, the processor configuration is invalid.

1\. To troubleshoot, hover over one of the processors, for instance the **GetFile** processor, and a warning message will appear. This message informs us of the requirements needed, so we can run this processor.

![error_getFile_processor_nifi_lab1](assets/tutorial-1-nifi-dataflow-automation-concepts/warning_getFile_processor_nifi.png)

For instance, the warning message indicates: we need to specify a directory path to tell the processor where to pull data and a connection for the processor to establish a relationship.

### Running the NiFi DataFlow

Once we finish connecting and configuring the components in our dataflow, there are at least 3 conditions we should check to ensure our dataflow successfully runs. We must verify that all relationships are established, the components are valid, stopped, enabled and have no active tasks. After you complete the verification process, you can select the processors, click the play symbol ![start_button_nifi](assets/tutorial-1-nifi-dataflow-automation-concepts/start_button_nifi.png) in the actions toolbar to run the dataflow. View the image of a dataflow that is active.

![run_dataflow_lab1_nifi_learn_ropes_concepts_section](assets/tutorial-1-nifi-dataflow-automation-concepts/run_dataflow_concepts.png)

## The Core Concepts of NiFi

When we learned the process of building a dataflow, we crossed paths with many of the core concepts of NiFi. You may be wondering what is the meaning behind a FlowFile, processor, connection, and other terms? Let's learn briefly about these terms because they will appear throughout the tutorial series. We want you to have the best experience in the tutorial. **Table 1** summarizes each term.

**Table 1**: NiFi Core Concepts

| NiFi Term  | Description  |
|:---|---:|
| `FlowFile`  | `Data brought into NiFi that moves through the system. This data holds attributes and can contain content.` |
| `Processor` | `Tool that pulls data from external sources, performs actions on attributes and content of FlowFiles and publishes data to external source.` |
| `Connection`  | `Linkage between processors that contain a queue and relationship(s) that effect where data is routed.` |
| `Back Pressure`  | `Prevents the system from being overrun with data by controlling the quantity or data size of FlowFiles that can be stored in the queue.` |
| `Flow Controller` | `Acts as a Broker to facilitate the exchange of FlowFiles between processors.` |
| `Process Group` | `Enables the creation of new components based on the composition of processors, funnels, etc.` |
| `Data Provenance` | `History of actions that occur on the data as it moves throughout the flow. Enables users to check the data from any processor or component while the FlowFiles move throughout the dataflow.` |
| `Controller Service` | A package of configuration parameters and code doing something in the background: Tells record processors how to interpret data, establishes connection parameters to external services (databases, APIs), sends statistics about NiFi to a monitoring service and sharing a state with cache services. |

## NiFi Architecture

Let's dive deeper into the infrastructure that enables NiFi to perform so well when it comes to building DataFlows that address various use cases.

![nifi_architecture_diagram](assets/tutorial-1-nifi-dataflow-automation-concepts/nifi_architecture_diagram.png)

NiFi executes within the Java Virtual Machine (JVM) located in the Host Operating System (OS/Host).

Inside the JVM,

- The Web Server enables us to access NiFi's User Interface from the web browser.
- The Flow Controller acts as the brains of the operation, it gives threads to extensions (custom processors), and keeps track of all the operations being executed by extensions
- The FlowFile Repository is the area NiFi keeps track of all the status updates regarding FlowFiles as they travel throughout the DataFlow
- The Content Repository is the location the content bytes of FlowFiles reside.
- The Provenance Repository consists of all the provenance event data.

## A Brief History of NiFi

Apache NiFi originated from the NSA Technology Transfer Program in Autumn of 2014. NiFi became an official Apache Project in July of 2015. NiFi has been in development for 8 years. NiFi was built with the idea to make it easier for people to automate and manage data-in-motion without having to write numerous lines of code. Therefore, the user interface comes with pallet of data flow components that can be dropped onto the graph and connected together. NiFi was also created to solve many challenges of data-in-motion, such as multi-way dataflows, data ingestion from any data source, data distribution with the required security and governance. NiFi can be used by a wide variety of users who come from a variety of backgrounds(development, business) and want to tackle the challenges stated above.

### Further Reading

The topics covered in the concepts section were brief and tailored toward the tutorial series.

- If you are interested in learning more in depth about these concepts, view [Getting Started with NiFi](https://nifi.apache.org/docs/nifi-docs/html/getting-started.html).
- [Credit card fraud prevention on a connected data platform](https://hortonworks.com/blog/credit-card-fraud-prevention-on-a-connected-data-platform/)
- [Qualcomm, Hortonworks showcase connected car platform at tu-automotive detroit](https://hortonworks.com/blog/qualcomm-hortonworks-showcase-connected-car-platform-tu-automotive-detroit/)
- [Cybersecurity: conceptual architecture for analytic response](https://hortonworks.com/blog/cybersecurity-conceptual-architecture-for-analytic-response/)


---
title: Launch NiFi HTML UI
---

# 2.Launch NiFi HTML UI

## Introduction

With the Hortonworks DataFlow (HDF) Sandbox, Apache NiFi comes pre-installed in the Ambari Stack and pre-configured out of the box to utilize many of its features. In the tutorial, it shows you how to access the NiFi HTML UI in one of two ways: use the HDF Splash Screen Page "Advanced Quick Links" or Ambari UI "Quick Links".

## Prerequisites

- Completed the prior tutorials within this tutorial series
- Downloaded and deployed the [Hortonworks DataFlow (HDF)](https://www.cloudera.com/downloads/hortonworks-sandbox/hdf.html?utm_source=mktg-tutorial) Sandbox
- Added HDF Sandbox Hostname to Your Hosts File, refer to [Learning the Ropes of the HDF sandbox](https://hortonworks.com/tutorial/getting-started-with-hdf-sandbox/)

## Outline

- [Step 1: Open HDF Splash Page](#step-1-open-hdf-splash-page)
- [Approach 1: Access NiFi HTML UI via Ambari Dashboard](#approach-1-access-nifi-html-ui-via-ambari-dashboard)
- [Approach 2: Launch NiFi HTML UI from HDF Splash Quick Links](#approach-2-launch-nifi-html-ui-from-hdf-splash-quick-links)
- [Summary](#summary)

> Note: For VMware users, you will need to add the auto generated IP address on startup of your virtual machine followed by HDF Sandbox hostname to your hosts file, example `192.168.17.129 sandbox-hdf.hortonworks.com`. On MAC, open `/private/etc/hosts`; LINUX, open `/etc/hosts`; WINDOWS 10, open `C:\Windows\System32\drivers\etc\hosts` as an administrator.

### Step 1: Open HDF Splash Page

1\. Open `sandbox-hdf.hortonworks.com:1080` with your favorite web browser:

![hdf_splash_screen](assets/tutorial-2-launch-nifi-html-ui/hdf_splash_screen.jpg)

Choose either approach to access NiFi UI.

### Approach 1: Access NiFi HTML UI via Ambari Dashboard

1\. Select the **LAUNCH DASHBOARD** button

2\. Type `raj_ops/raj_ops` to login to Ambari.

![login_ambari_ui](assets/tutorial-2-launch-nifi-html-ui/approach-1-ambari/login_ambari_ui.jpg)

3\. Select the **NiFi Service**, and click on **NiFi UI**:

![open-nifi-ui-via-ambari.png](assets/tutorial-2-launch-nifi-html-ui/approach-1-ambari/open-nifi-ui-via-ambari.jpg)

4\. The NiFi UI comes with a default Trucking-IoT NiFiFlow as seen on the image below. To delete the default NiFiFlow and work with a new canvas select the NiFiFlow (cltr+A) then right click and select delete.

![delete_trucking_iot_nififlow](assets/tutorial-2-launch-nifi-html-ui/approach-1-ambari/trucking_iot_delete_flow.jpg)

Sample of a clean NiFi Canvas:
![open_nifi_html_interface.png](assets/tutorial-2-launch-nifi-html-ui/approach-1-ambari/open_nifi_html_interface.jpg)

### Approach 2: Launch NiFi HTML UI from HDF Splash Quick Links

1\. Select the **QUICK LINKS** button

2\. Hover over the **NiFi** box and select **Go to UI**

![splash_nifi_quicklink](assets/tutorial-2-launch-nifi-html-ui/approach-2-splash/splash_nifi_quicklink.jpg)

3\. The NiFi UI comes with a default Trucking-IoT NiFiFlow as seen on the image below. To delete the default NiFiFlow and work with a new canvas select the NiFiFlow (cltr+A for Mac) then right click and select delete.

![delete_trucking_iot_nififlow](assets/tutorial-2-launch-nifi-html-ui/approach-1-ambari/trucking_iot_delete_flow.jpg)

Sample of a clean NiFi Canvas:
![open_nifi_html_interface.png](assets/tutorial-2-launch-nifi-html-ui/approach-2-splash/open_nifi_html_interface.jpg)

## Summary

Congratulations! You explored two approaches for launching NiFi UI. You opened the HDF Splash Page: Approach 1 was launching the Ambari Dashboard from the "New To HDF path" while Approach 2 was using the NiFi Quick Link from the "Advanced HDF path". Now you are ready to explore the next tutorial to began building our simple dataflow.




---
title: Build a NiFi Process Group to Simulate NextBus API
---

# 3.Build a NiFi Process Group to Simulate NextBus API

## Introduction

You will build a NiFi DataFlow and package it into a process group to simulate the NextBus API transit feed and check the data generating from the simulator.

![sf_ocean_view_route_nifi_streaming](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/live_stream_sf_muni_nifi_learning_ropes.png)

**Figure 1:** Here is a visualization of the transit data you will be ingesting into NiFi.

## Prerequisites

- Completed the prior tutorials within this tutorial series

## Outline

- [Approach 1: Build SimulateXmlTransitEvents Process Group](#approach-1-build-simulatexmltransitevents-process-group)
- [Step 1: Create a Process Group](#step-1-create-a-process-group)
- [Step 2: Add GetFile to Ingest NextBus Data Seed](#step-2-add-getfile-to-ingest-nextbus-data-seed)
- [Step 3: Add UnpackContent to Decompress the Zipped Data](#step-3-add-unpackcontent-to-decompress-the-zipped-data)
- [Step 4: Add ControlRate to Regulate Data Flow Speed](#step-4-add-controlrate-to-regulate-data-flow-speed)
- [Step 5: Add UpdateAttribute to Make Each FlowFile Name Unique](#step-5-add-updateattribute-to-make-each-flowfile-name-unique)
- [Approach 2: Import NiFi SimulateXmlTransitEvents Process Group](#approach-2-import-nifi-simulatexmltransitevents-process-group)
- [Summary](#summary)
- [Further Reading](#further-reading)

If you prefer to build the dataflow manually step-by-step, continue on to **Approach 1**. Else if you want to see the NiFi flow in action within minutes, refer to **Approach 2**.

## Approach 1: Build SimulateXmlTransitEvents Process Group

### 1.1 Create Label for Process Group

1\. Go to the **components** toolbar, drag and drop the Label icon ![label_icon](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/label_icon.png) onto the NiFi canvas.

![label_generic](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/label_generic.png)

2\. Click on the right bottom corner and stretch the label over approximately 24 squares.

![stretch_label](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/stretch_label.png)

3\. Right click, select configure and name it `Generate Transit Location via Data Seed based on NextBus API`. Change the font size to `18px`.

![label_for_simulatexmltransitevents](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/label_for_simulatexmltransitevents.png)

### Step 1: Create a Process Group

1\. Go to the **components** toolbar, drag and drop the **Process Group** icon ![process_group](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/process_group.png) onto the NiFi canvas. Name it `SimulateXmlTransitEvents`. Press **ADD**.

![SimulateXmlTransitEvents](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/SimulateXmlTransitEvents.png)

2\. Double click on new process group. At the bottom left corner, breadcrumbs will show you entered **SimulateXmlTransitEvents** Process Group.

![breadcrumb_SimulateXmlTransitEvents](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/breadcrumb_SimulateXmlTransitEvents.png)

### Step 2: Add GetFile to Ingest NextBus Data Seed

1\. Add the processor icon ![processor_nifi_iot](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/processor_nifi_iot.png) onto the graph.

2\. Select the **GetFile** processor and press **ADD**.

![getfile_processor](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/getfile_processor.png)

- Creates FlowFiles from files in a directory. NiFi will ignore files it doesn’t have read permissions for.

3\. Right click on the **GetFile** processor, click **configure** from dropdown menu

![configure_processor_nifi_iot](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/configure_processor_nifi_iot.png)

4\. Enter the **Properties** tab. Add the properties listed in **Table 1** to update the processor's appropriate properties. Press **OK** after changing a property.

**Table 1:** Update **GetFile** Properties Tab

| Property  | Value  |
|:---|---:|
| `Input Directory`  | `/sandbox/tutorial-files/640/nifi/input`  |
| `Keep Source File`  | `true`  |

- **Input Directory** location at which data is ingested into the dataflow

- **Keep Source File** source files in directory remain after all data is ingested

5\. Once each property is updated, go to the **Scheduling** tab, add the configuration information listed in **Table 2**.

**Table 2:** Update **GetFile** Scheduling Tab

| Name  | Value  |
|:---|---:|
| `Run Schedule`  | `6 sec`  |

- **Run Schedule** the processor executes a task every 6 seconds

6\. Click **Apply**.

### Step 3: Add UnpackContent to Decompress the Zipped Data

1\. Add the **UnpackContent** processor onto the NiFi canvas.

2\. Connect **GetFile** to **UnpackContent** processor by dragging the arrow icon from the first processor to the next. When the Create Connection window appears, verify **success** checkbox is checked, else check it. Click **Add**.

![arrow_icon](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/arrow_icon.png)

**Figure 2:** Arrow Icon Appears When Hovering Over **GetFile** Processor

![drag_arrow_icon_connect](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/drag_arrow_icon_connect.png)

**Figure 3:** Drag Arrow Icon to Connect two Processors (GetFile -> UnpackContent)

![connection_window_checkbox](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/connection_window_checkbox.png)

**Figure 4:** "Create Connection" Window for GetFile -> "UnpackContent" -> **success** checkbox checked.

3\. Open the **UnpackContent** processor configuration **properties** tab. Add the properties listed in **Table 3** to update the processor's appropriate properties.

**Table 3:** Update **UnpackContent** Properties Tab

| Property  | Value  |
|:---|---:|
| `Packaging Format`  | `zip`  |

- **Packaging Format** tells the processor of packaging format used to decompress the file

4\. Once each property is updated, enter the **Scheduling** tab, add the configuration information listed in **Table 4**.

**Table 4:** Update **UnpackContent** Scheduling Tab

| Name  | Value  |
|:---|---:|
| `Run Schedule`  | `1 sec`  |

- **Run Schedule** the processor executes a task every 1 second and avoids back pressure downstream to the next processor

5\. Open the processor config **Settings** tab, under Auto terminate relationships, check the **failure** and **original** checkboxes. Click **Apply**.

![unpackcontent_settings_tab_original_failure](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/unpackcontent_settings_tab_original_failure.png)

**Figure 5:** UnpackContent Settings Tab Window

### Step 4: Add ControlRate to Regulate Data Flow Speed

1\. Add the **ControlRate** processor onto the NiFi canvas.

2\. Connect **UnpackContent** to **ControlRate** processor. When the Create Connection window appears, verify **success** checkbox is checked, else check it. Click **Add**.

![unpackcontent_to_controlrate](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/unpackcontent_to_controlrate.png)

**Figure 6:** Connect UnpackContent to ControlRate

3\. Open the processor configuration **properties** tab. Add the properties listed in **Table 5** to update the processor's appropriate properties.

**Table 5:** Update ControlRate Properties Tab

| Property  | Value  |
|:---|---:|
| `Rate Control Criteria`  | `flowfile count`  |
| `Maximum Rate`  | `20`  |
| `Time Duration`  | `6 second`  |

- **Rate Control Criteria** instructs the processor to count the number of FlowFiles before a transfer takes place

- **Maximum Rate** instructs the processor to transfer 20 FlowFiles at a time

- **Time Duration** makes it so only 20 FlowFiles will transfer through this processor every 6 seconds.

4\. Open the processor config **Settings** tab, under Auto terminate relationships, check the **failure** checkbox. Click **Apply**.

### Step 5: Add UpdateAttribute to Make Each FlowFile Name Unique

1\. Add the **UpdateAttribute** processor onto the NiFi canvas.

2\. Connect **ControlRate** to **UpdateAttribute**. When the Create Connection window appears, verify **success** checkbox is checked, else check it. Click **Add**.

![controltate_to_updateattribute](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/controltate_to_updateattribute.png)

**Figure 7:** Connect ControlRate to UpdateAttribute

3\. Add a new dynamic property for NiFi expression, click on the plus button **"+"** in the top right corner. Insert the following property name and value into your properties tab as shown in the table below:

**Table 6:** Add UpdateAttribute Properties Tab

| Property  | Value  |
|:---|---:|
| `filename`  | `transit-data-${UUID()}.xml`  |

![updateattribute_property](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/updateattribute_property.png)

**Figure 8:** UpdateAttribute Property Tab Window

- **filename** updates each FlowFile with a unique identifier

4\. Click **Apply**.

### Step 6: Add PutFile to Store RawTransitEvents to Disk

1\. Add the **PutFile** processor onto the NiFi canvas.

2\. Connect **UpdateAttribute** to **PutFile**. When the Create Connection window appears, verify **success** checkbox is checked, else check it. Click **Add**.

![updateattribute_to_putfile](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/updateattribute_to_putfile.png)

**Figure 9:** Connect **UpdateAttribute** to **PutFile**

3\. Open the processor configuration **properties** tab. Add the property listed in **Table 7** to update the processor's appropriate properties.

**Table 7:** Update PutFile Properties Tab

| Property  | Value  |
|:---|---:|
| `Directory`  | `/sandbox/tutorial-files/640/nifi/output/rawtransitevents`  |

4\. Open the processor config **Settings** tab, under Auto terminate relationships, check the **failure** and **success** checkboxes. Click **Apply**.

### Step 7: Add Output Port for External Component Connection

1\. Add the **Output Port** ![output_port](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/output_port.png) component onto the NiFi canvas. Name it `RawTransitEvents`.

2\. Connect **UpdateAttribute** to **RawTransitEvents** output port. When the Create Connection window appears, verify **success** checkbox is checked, else check it. Click **Add**.

![simulatexmltransitevents_flow](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/simulatexmltransitevents_flow.png)

**Figure 10:** Connect **UpdateAttribute** to **Output Port (RawTransitEvents)**

### Step 8: Check Data Stored to Local File System Via Web Shell Client

1\. With the NiFi DataFlow unselected, hit the **start** button ![start_button_nifi_iot](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/start_button_nifi_iot.png) located in the Operate Palette to activate the **SimulateXmlTransitEvents** process group dataflow.

2\. Let the flow run for about 1 minute, then stop the flow by hitting the **stop** button.

3\. Launch Sandbox Web Shell Client via Hortonworks DataFlow (HDF) Splash Screen from **[Advanced HDF SB Quick Links](http://sandbox-hdf.hortonworks.com:1080)** Link.

**Figure 11:** SB Quick Link for Web Shell Client

> Note: Username is "root", initial password is "hadoop".

![web_shell_client](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/web_shell_client.jpg)

You have now SSH'd into the HDF Sandbox Server.

4\. Navigate to the output directory in which the transit data is being written to:

~~~bash
cd /sandbox/tutorial-files/640/nifi/output/rawtransitevents
~~~

5\. Run the `ls` command to list files in the current directory:

~~~bash
ls
~~~

6\. Run the `cat` command to output the content of the data file to the console:

~~~bash
cat transit-data-{flowfile-UUID}.xml
~~~

![check_data_written_to_fs](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/check_data_written_to_fs.png)

**Figure 12:** Output of Transit Data in Web Shell Client

## Approach 2: Import NiFi SimulateXmlTransitEvents Process Group

1\. Download the [tutorial-3-nifi-flow-SimulateXmlTransitEvents.xml](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/template/tutorial-3-SimulateXmlTransitEvents.xml) template file.

2\. Use the template icon ![nifi_template_icon](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/nifi_template_icon.png) located in the Operate Palette.

3\. **Browse**, find the template file, click **Open** and hit **Upload**.

4\. From the **Components Toolbar**, drag the template icon ![nifi_template_icon](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/add_nifi_template.png) onto the graph and select the **tutorial-3-nifi-flow-SimulateXmlTransitEvents.xml** template file.

5\. Right click on the process group, hit the **start** button ![start_button_nifi](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/start_button_nifi_iot.png) to activate the dataflow.

![run_dataflow_lab1_nifi_learn_ropes](assets/tutorial-3-build-a-nifi-process-group-to-simulate-nextbus-api/complete_simulatetransitdata_pg.png)

**Figure 13:** NiFi Flow that pulls in San Francisco Muni Transit Events from the XML Simulator, stores the raw transit events to the local file system and also sends the data out to the rest of the flow through the output port.

The building blocks of every dataflow consist of processors. When sections of the flow become complex, you can start grouping processors inside process groups. These tools perform actions on data to ingest, route, extract, split, aggregate or store it. Our dataflow process group above contains processors, each processor includes a high level description of their role in the tutorial:

- **SimulateXmlTransitEvents (Process Group)**
  - **GetFile** fetches the vehicle location simulator data for files in a directory.
  - **UnpackContent** decompresses the contents of FlowFiles from the traffic simulator zip file.
  - **ControlRate** controls the rate at which FlowFiles are transferred to follow-on processors enabling traffic simulation.
  - **UpdateAttribute** renames every FlowFile to give them unique names
  - **PutFile** stores data to local file system
  - **Output Port** makes the connection for the process group to connect to other components (process groups, processors, etc)


Refer to [NiFi's Documentation](https://nifi.apache.org/docs.html) to learn more about each processor described above.

### Summary

Congratulations! You just built a NiFi **SimulateXmlTransitEvents** process group to replicate the NextBus API, which generates transit data for passengers. You learned to use **GetFile** processor to ingest the transit data seed. The **UnpackContent** processor decompressed the transit data seed zip file and routed the data onto the rest of the flow. **ControlRate** processor controlled the rate at which each FlowFile was distributed to the remaining components of the flow. The **Output Port** allows for FlowFiles to be dispersed to the next process group you will learn to build in the next tutorial called **ParseTransitEvents**.

### Further Reading

- [Process Group](https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#process_group_anatomy)
- [GetFile](https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-standard-nar/1.5.0/org.apache.nifi.processors.standard.GetFile/index.html)
- [UnpackContent](https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-standard-nar/1.5.0/org.apache.nifi.processors.standard.UnpackContent/index.html)
- [ControlRate](https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-standard-nar/1.5.0/org.apache.nifi.processors.standard.ControlRate/index.html)
- Under "Adding Components to the Canvas," head to [Output Port](https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#adding-components-to-the-canvas)


---
title: Build a NiFi Process Group to Parse Transit Events
---

# 4.Build a NiFi Process Group to Parse Transit Events

## Introduction

You will learn to build a portion of the NiFi DataFlow in a process group to parse data using `XPath Expression Language`for **timestamp, vehicle location, speed, vehicle ID** and **other data** from the **San Francisco Muni Transit Simulator**.

## Prerequisites

- Completed the prior tutorials within this tutorial series

## Outline

- [Approach 1: Manually Build ParseTransitEvents Process Group (PG)](#approach-1-manually-build-parsetransitevents-process-group-pg)
- [Step 1: Create a Process Group (PG)](#step-1-create-a-process-group-pg)
- [Step 2: Add an Input Port to Ingest Data Into this PG](#step-2-add-an-input-port-to-ingest-data-into-this-pg)
- [Step 3: Add EvaluateXPath to Extract the Timestamp for Transit Observation](#step-3-add-evaluatexpath-to-extract-the-timestamp-for-transit-observation)
- [Step 4: Add SplitXml to Split A FlowFile into Multiple Separate FlowFiles](#step-4-add-splitxml-to-split-a-flowfile-into-multiple-separate-flowfiles)
- [Step 5: Add EvaluateXPath to Extract Transit Observations](#step-5-add-evaluatexpath-to-extract-transit-observations)
- [Step 6: Add an Output Port to Route Data Outside this PG](#step-6-add-an-output-port-to-route-data-outside-this-pg)
- [Step 7: Connect SimulateXmlTransitEvents to ParseTransitEvents](#step-7-connect-simulatexmltransitevents-to-parsetransitevents)
- [Step 8: Verify ParseTransitEvents Extracts Values From SimulateXmlTransitEvents](#step-8-verify-parsetransitevents-extracts-values-from-simulatexmltransitevents)
- [Approach 2: Import ParseTransitEvents Process Group](#approach-2-import-parsetransitevents-process-group)
- [Summary](#summary)
- [Further Reading](#further-reading)

If you prefer to build the dataflow manually step-by-step, continue on to **Approach 1**. Else if you want to see the NiFi flow in action within minutes, refer to **Approach 2**.

### Approach 1: Manually Build ParseTransitEvents Process Group (PG)

### 1.1 Create Label for Process Group

1\. Return to the NiFi root level by clicking on the Nifi Flow label at the lower left corner:

![back-root](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/back-root.jpg)

2\. Add a Label ![label_icon](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/label_icon.png) onto the NiFi canvas for the new process group. Right click, **Change color** to **Blue**.

3\. Right click, select configure and name it `Extract Transit Key Values pairs (Timestamp, Direction, Latitude, Longitude, Vehicle_ID, Vehicle_Speed) From XML FlowFiles using XPath Expression`. Choose Font Size to `18px`.

![label_for_simulatexmltransitevents](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/label_parsetransitevents.png)

### Step 1: Create a Process Group (PG)

1\. Add a new Process Group onto the NiFi canvas and name it `ParseTransitEvents`

![ParseTransitEvents](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/ParseTransitEvents.png)

2\. Double click on the new Process Group to enter it.

![ParseTransitEvents](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/breadcrumbs_ParseTransitEvents.png)

### Step 2: Add an Input Port to Ingest Data Into this PG

1\. Add the **Input Port** ![output_port](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/input_port.png) component onto the NiFi canvas. Name it `IngestRawTransitEvents`.

![IngestRawTransitEvents](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/IngestRawTransitEvents.png)

### Step 3: Add EvaluateXPath to Extract the Timestamp for Transit Observation

1\. Add the **EvaluateXPath** processor onto the NiFi canvas.

2\. Connect **IngestRawTransitEvents** input port to **EvaluateXPath** processor. When the Create Connection window appears, click **Add**.

![ingestrawtransitevents_to_evaluatexpath](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/ingestrawtransitevents_to_evaluatexpath.png)

**Figure 1:** Connect **IngestRawTransitEvents** to **EvaluateXPath**

3\. Open the processor configuration **properties** tab. Add the properties listed in **Table 1** and if the original properties have values, update them. For the second property in **Table 1**, add a new dynamic property for XPath expression, select the plus **( + )** button. Insert the following property name and value into your properties tab as shown in the table below:

**Table 1:** Update **EvaluateXPath** Properties Tab

| Property  | Value  |
|:---|---:|
| `Destination`  | `flowfile-attribute`  |
| `Last_Time`  | `//body/lastTime/@time`  |

- **Destination** result from XPath evaluation stored into FlowFile attribute

- **Last_Time** is a FlowFile Attribute and XPath expression that retrieves value of time node in the XML file

![evaluateXPath_config_property_tab_window](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/evaluateXPath_config_property_tab_window.jpg)

**Figure 2:** EvaluateXPath Configuration Property Tab Window

4\. Open the processor config **Settings** tab, change the name to **ExtractTimestamp**, then under Auto terminate relationships, check the **failure** and **unmatched** checkboxes. Click **Apply**.

### Step 4: Add SplitXml to Split A FlowFile into Multiple Separate FlowFiles

1\. Add the **SplitXml** processor onto the NiFi canvas.

2\. Connect **ExtractTimestamp** to **SplitXML** processor. When the Create Connection window appears, verify **matched** checkbox is checked, else check it. Click **Add**.

![evaluatexpath_to_splitxml](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/evaluatexpath_to_splitxml.png)

**Figure 3:** Connect **EvaluateXPath** to **SplitXML**

3\. Keep **SplitXML** default configuration **properties**.

4\. Since each property is updated, navigate to the **Scheduling tab** and change the **Run Schedule** from 0 sec to `1 sec`, so the processor executes a task every 1 second.

5\.  Open the processor config **Settings** tab, under Auto terminate relationships, check the **failure** and **original** checkboxes. Click **Apply**.

### Step 5: Add EvaluateXPath to Extract Transit Observations

1\. Add the **EvaluateXPath** processor onto the NiFi canvas.

2\. Connect **SplitXML** to **EvaluateXPath** processor. When the Create Connection window appears, verify **split** checkbox is checked, if not check it. Click Add.

![splitxml_to_evaluatexpath](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/splitxml_to_evaluatexpath.png)

**Figure 4:** Connect **SplitXML** to **EvaluateXPath**

3\. Open the processor configuration **properties** tab. Add the properties listed in **Table 2** and if the original properties have values, update them. For the remaining properties in **Table 2**, add new dynamic properties for XPath expressions, click on the **( + )** button. Insert the following property name and value into your properties tab as shown in the table below:

**Table 2:** Update **EvaluateXPath** Properties Tab

| Property  | Value  |
|:---|---:|
| `Destination`  | `flowfile-attribute`  |
| `Direction_of_Travel`  | `//vehicle/@dirTag`  |
| `Latitude`  | `//vehicle/@lat`  |
| `Longitude`  | `//vehicle/@lon`  |
| `Vehicle_ID`  | `//vehicle/@id`  |
| `Vehicle_Speed`  | `//vehicle/@speedKmHr`  |

- **Destination** set to FlowFile attribute because the result of values from XPath expressions need to be stored in FlowFile attributes.

- **5 user-defined attributes** each represent data related to transit observations associated with the timestamp extracted earlier.

4\. Open the processor config **Settings** tab, change the name to **ExtractTransitObservations**, then under Auto terminate relationships, check the **failure** and **unmatched** checkboxes. Click **Apply**.

### Step 6: Add an Output Port to Route Data Outside this PG

1\. Add the **Output Port** ![output_port](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/output_port.png) component onto the NiFi canvas. Name it `ParsedTransitEvents`.

2\. Connect **ExtractTransitObservations** to **ParsedTransitEvents** output port. When the Create Connection window appears, verify **matched** checkbox is checked, if not check it. Click Add.

![ParseTransitEvents_dataflow_pg](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/ParseTransitEvents_dataflow_pg.png)

**Figure 5:** Connect **ExtractTransitObservations** to **ParsedTransitEvents**

### Step 7: Connect SimulateXmlTransitEvents to ParseTransitEvents

1\. Re-enter the `NiFi Flow` breadcrumb.

![re_enter_nifi_flow_breadcrumb](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/re_enter_nifi_flow_breadcrumb.png)

2\. Connect **SimulateXmlTransitEvents** to **ParseTransitEvents** process group. When the Create Connection window appears, verify **From Output = RawTransitEvents** and connects to **To Input = IngestRawTransitEvents**. Click **Add**.

![SimulateXmlTransitEvents_to_ParseTransitEvents](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/parsetransitevents_pg.png)

**Figure 6:** Connection of **SimulateXmlTransitEvents** and **ParseTransitEvents** process group.

### Step 8: Verify ParseTransitEvents Extracts Values From SimulateXmlTransitEvents

1\. Hold **Shift**, press on the mouse and hover over process groups to select them, then hit the **start** button ![start_button_nifi_iot](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/start_button_nifi_iot.png) located in the **Operate** Palette to activate the dataflow.

![parsetransitevents_pg](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/started_parsetransitevents_pg.png)

2\. Double click on **ParseTransitEvents** process group. Right click on the **ExtractTransitObservations** processor, select **View Data Provenance**.

![extracttransitobservations_data_provenance](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/extracttransitobservations_data_provenance.png)

3\. View any event by selecting the view provenance event icon ![i_symbol_nifi](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/i_symbol_nifi_lab1.png)

4\. Pick a row from the list of events, click on the "i" to the left of that row. Then click on the **Attributes** tab. Select **Show modified attributes only** to display only the attributes being parsed with **XPath expression language**. As long as you see values mapped to their attribute name, you have verified the processor is extracting XML data from the flowfile successfully.

![verify_evaluateXPath_extracts_data](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/verify_evaluateXPath_extracts_data.png)

**Figure 7:** ExtractTransitObservations parses the XML data for **Direction_of_Travel**, **Latitude**, **Longitude**, **Vehicle_ID**, **Vehicle_Speed**.

### Approach 2: Import ParseTransitEvents Process Group

**Warning: If you Imported the Previous DataFlow, Read these steps first**

1\. In your process group, if there are any **queues** left with data, remove the data. Right click the queue, select **Empty queue**, click **EMPTY**.

2\. Navigate back to the **NiFi Flow** breadcrumb level to delete your flow.

To delete your previous flow, hold **command** or **ctrl** and press **A** to highlight your dataflow, then press **delete** or **backspace**.

**Import the New DataFlow Template**

3\. Download the [tutorial-4-ParseTransitEvents.xml](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/template/tutorial-4-ParseTransitEvents.xml) template file.

4\. Use the template icon ![nifi_template_icon](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/nifi_template_icon.png) located in the **Operate** Palette.

5\. **Browse**, find the template file, click **Open** and hit **Upload**.

6\. From the **Components Toolbar**, drag the template icon ![nifi_template_icon](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/add_nifi_template.png) onto the graph and select the **tutorial-4-ParseTransitEvents.xml** template file.

7\. Hit the **start** button ![start_button_nifi_iot](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/start_button_nifi_iot.png) to activate the dataflow.

![run_dataflow_lab1_nifi_learn_ropes](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/started_parsetransitevents_pg.png)

**Figure 8:** **tutorial4-ParseTransitEvents.xml** template includes a NiFi Flow that pulls in San Francisco Muni Transit Events from the XML Simulator, parses through the data to extract key values and stores the transit observations as a JSON file.

Overview of the Process Groups and their Processors:

- **SimulateXmlTransitEvents (Process Group)**
  - **GetFile** fetches the vehicle location simulator data for files in a directory.
  - **UnpackContent** decompresses the contents of FlowFiles from the traffic simulator zip file.
  - **ControlRate** controls the rate at which FlowFiles are transferred to follow-on processors enabling traffic simulation.
  - **UpdateAttribute** renames every FlowFile to give them unique names
  - **PutFile** stores data to local file system
  - **Output Port** makes the connection for the process group to connect to other components (process groups, processors, etc)

- **ParseTransitEvents (Process Group)**
  - **Input Port** ingests data from SimulateXmlTransitEvents Process Group
  - **ExtractTimestamp** extracts the timestamp of the last update for vehicle location data returned from each FlowFile.
  - **SplitXML** splits the parent's child elements into separate FlowFiles. Since vehicle is a child element in our xml file, each new vehicle element is stored separately.
  - **ExtractTransitObservations** extracts attributes: vehicle id, direction, latitude, longitude and speed from vehicle element in each FlowFile.
  - **Output Port** outputs data with the new FlowFile attribute (key/values) to the rest of the flow

Refer to [NiFi's Documentation](https://nifi.apache.org/docs.html) to learn more about each processor described above.

### Summary

Congratulations! You just built a NiFi **ParseTransitEvents** process group to parse the XML content and extract transit observations into FlowFile attributes. The **Input Port** pulls data from the **SimulateXmlTransitEvents** process group, which goes into an **ExtractTimestamp** processor to pull out the **timestamp** for the vehicle observation and add that timestamp as a FlowFile attribute. The FlowFile content rows are then split into multiple FlowFiles via **SplitXml** processor. These single FlowFile records are routed to another **ExtractTransitObservations** processor to extract **transit observations** for multiple transit vehicles of that timestamp from earlier. This data with new FlowFile attributes is routed to the rest of the flow via **Output Port**.

### Further Reading

-   [XPath Expression Tutorial](http://www.w3schools.com/xml/xpath_intro.asp)



---
title: Build a NiFi Process Group to Validate the GeoEnriched Data
---

# 5.Build a NiFi Process Group to Validate the GeoEnriched Data

## Introduction

With the transit data being pulled from NextBus API simulator, it shows location in form of latitude and longitude, but it does not indicate more meaningful insights like **neighborhoods** that the transits pass by on their routes. You will add that capability to the NiFi flow by integrating Google Places API. Additionally you will add the capability to automate validation of the geo enriched data. You will add a new process group called **ValidateGeoEnrichedTransitData**.

## Prerequisites

- Completed the prior tutorials within this tutorial series

## Outline

- [Google Places API Basics](#google-places-api-basics)
- [Approach 1: Manually Build ValidateGeoEnrichedTransitData Process Group (PG)](#approach-1-manually-build-validategeoenrichedtransitdata-process-group-pg)
- [Step 1: Obtain API Key for NiFi's InvokeHTTP Processor](#step-1-obtain-api-key-for-nifis-invokehttp-processor)
- [Step 2: Create Process Group and Label For It](#step-2-create-process-group-and-label-for-it)
- [Step 3: Add an Input Port to Ingest Data Into this PG](#step-3-add-an-input-port-to-ingest-data-into-this-pg)
- [Step 4: Add RouteOnAttribute to Validate NextBus Simulator Data](#step-4-add-routeonattribute-to-validate-nextbus-simulator-data)
- [Step 5: Add InvokeHTTP to Pull GeoEnriched Data from Google Places API](#step-5-add-invokehttp-to-pull-geoenriched-data-from-google-places-api)
- [Step 6: Add EvaluateJsonPath to Extract GeoEnriched Transit Data](#step-6-add-evaluatejsonpath-to-extract-geoenriched-transit-data)
- [Step 7: Add RouteOnAttribute to Validate Google Places Data](#step-7-add-routeonattribute-to-validate-google-places-data)
- [Step 8: Add an Output Port to Route Data Outside this PG](#step-8-add-an-output-port-to-route-data-outside-this-pg)
- [Step 9: Connect ParseTransitEvents to ValidateGeoEnrichedTransitData](#step-9-connect-parsetransitevents-to-validategeoenrichedtransitdata)
- [Step 10: Run the DataFlow](#step-10-run-the-dataflow)
- [Step 11: Verify GeoEnriched Data Routed by ValidateGooglePlacesData is Valid](#step-11-verify-geoenriched-data-routed-by-validategoogleplacesdata-is-valid)
- [Approach 2: Import ValidateGeoEnrichedTransitData Process Group](#approach-2-import-enriched-nifi-flow-via-places-api)
- [Summary](#summary)
- [Further Reading](#further-reading)

If you prefer to build the dataflow manually step-by-step, continue on to [Approach 1](#step-1-obtain-api-key-for-nifis-invokehttp-processor). Else if you want to see the NiFi flow in action within minutes, refer to [Approach 2](#step-2-create-process-group-and-label-for-it).

You will need to understand Google Places API, so that it will be easier to incorporate this API's data into the NiFi flow, which will be built in **Approach 1**.

## Google Places API Basics

Google Places API Web Service returns information about places: establishments, geographic locations and prominent points of interest based on Latitude and Longitude coordinates that are passed into HTTP requests. The Places API includes six place requests: **Place Searches**, **Place Details**, **Place Add**, **Place Photos**, **Place Autocomplete** and **Query Autocomplete**. Read more about these place requests in [Introducing the API](https://developers.google.com/places/web-service/intro).

All requests are accessed through an HTTP request and return either JSON or XML response.

What are the necessary components to use the Places API?

- https:// protocol
- API Key

## Approach 1: Manually Build ValidateGeoEnrichedTransitData Process Group (PG)

### Step 1: Obtain API Key for NiFi's InvokeHTTP Processor

For our dataflow, our task is to enrich the data by searching for neighborhoods within proximity of a vehicle's varying location. Currently location is represented as Latitude and Longitude coordinates. You will enhance these insights to retrieve two parameters from this location data: name of the neighborhoods and San Francisco Muni Transit Agency. So, you will integrate Nearby Search HTTP request with NiFi.

The Nearby Search request is an HTTP URL of the following definition, which we will need for NiFi:

~~~text
https://maps.googleapis.com/maps/api/place/nearbysearch/output?parameters
~~~

The `output` can come in two formats: `json` or `xml`. We will use json for this tutorial.

Let's obtain the **required parameters** to initiate a Nearby Search request.

1\. You will need to [obtain an API key](https://developers.google.com/places/web-service/get-api-key), so it can identify our application for quota management and places added from the application are instantly available to our app (NiFi).

2\. You will use a standard Google Places API. Click on the blue **Get A Key** button to activate the API Web Service.

3\. A window will appear that says **Enable Google Places API Web Service**. Select **Yes**. Then **Create And Enable API**. Wait a few seconds for the new window to load.

4\. Now a screen with your unique API key will appear similar to the screen below:

![api_key](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/api_key.png)

Now you have the API Key parameter for our HTTP request. We also have the other required parameters: **location** and **radius**, which can be a distance that does not surpass 50,000 meters. You can use one optional parameter **type** to signify what type of place you are interested in searching for.

5\. Let's build the HTTP URL with the parameters below, so we can insert the URL as a property value into **InvokeHTTP** later in the tutorial.

- API Key = AIzaSyDY3asGAq-ArtPl6J2v7kcO_YSRYrjTFug
- Latitude = ${Latitude}
- Longitude = ${Longitude}
- radius = 500
- type = neighborhood

~~~text
https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=${Latitude},${Longitude}&radius=500&type=neighborhood&key=AIzaSyDY3asGAq-ArtPl6J2v7kcO_YSRYrjTFug
~~~

> Note: Your **API Key** will be different than the one in the URL above. You will need to use the **ONE** you get from Google Places API Web Service.

### Step 2: Create Process Group and Label For It

1\. Add a Label ![label_icon](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/label_icon.png) onto the NiFi canvas for the new process group. Right click, **Change color** to **Purple**.

2\. Right click, select configure and name it `Geo Enrich via Google Places API and
Validate Transit Data for empty values`.

![label_for_ValidateGeoEnrichedTransitData](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/label_for_ValidateGeoEnrichedTransitData.png)

3\. Add a new Process Group onto the NiFi canvas and name it `ValidateGeoEnrichedTransitData`

![ValidateGeoEnrichedTransitData](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/ValidateGeoEnrichedTransitData.png)

4\. Double click on the new process group to enter it.

![ParseTransitEvents](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/breadcrumb_ValidateGeoEnrichedTransitData.png)

### Step 3: Add an Input Port to Ingest Data Into this PG

1\. Add the **Input Port** ![output_port](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/input_port.png) component onto the NiFi canvas. Name it `IngestParsedTransitEvents`.

![IngestParsedTransitEvents](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/IngestParsedTransitEvents.png)

### Step 4: Add RouteOnAttribute to Validate NextBus Simulator Data

1\. Add the **RouteOnAttribute** processor onto the NiFi canvas.

2\. Connect **IngestParsedTransitEvents** input port to **RouteOnAttribute** processor. When the Create Connection window appears, click **Add**.

![IngestParsedTransitEvents_to_RouteOnAttribute](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/IngestParsedTransitEvents_to_RouteOnAttribute.png)

**Figure 1:** Connect **IngestParsedTransitEvents** input port to **RouteOnAttribute** processor

3\. Open the processor configuration **properties** tab. Add a new dynamic property for NiFi expression, select the **( + )** button. Insert the following property name and value into your properties tab as shown in the table below:

**Table 1:** Add to **RouteOnAttribute** Properties Tab

| Property  | Value  |
|:---|:---|
| `ValidateTransitObservations`  | `${Direction_of_Travel:isEmpty():not():and(${Last_Time:isEmpty():not()}):and(${Latitude:isEmpty():not()}):and(${Longitude:isEmpty():not()}):and(${Vehicle_ID:isEmpty():not()}):and(${Vehicle_Speed:equals('0'):not()})}`|

~~~java
//Note: If after completing step 3 your processor shows a triangle with an exclamation mark stating that the ValidateTransitObservations is invalid, copy and paste the following into the value section of the property tab.

${Direction_of_Travel:isEmpty():not():and(
	${Last_Time:isEmpty():not()}):and(
		${Latitude:isEmpty():not()}):and(
			${Longitude:isEmpty():not()}):and(
				${Vehicle_ID:isEmpty():not()}):and(
					${Vehicle_Speed:equals('0'):not()}
)}

~~~

**Filter_Attributes** uses the FlowFile Attribute key values obtained from XPath Expressions to filter out any FlowFiles that either have at least one empty Attribute value or the speed attribute value equals 0. Else the FlowFiles are passed to the remaining processors.

4\. Open the processor config **Settings** tab, change the name from RouteOnAttribute to `ValidateNextBusData`. under Auto terminate relationships, check the **unmatched** checkbox. Click **Apply**.

### Step 5: Add InvokeHTTP to Pull GeoEnriched Data from Google Places API

1\. Add the **InvokeHTTP** processor onto the NiFi graph. Connect **ValidateNextBusData** to **InvokeHTTP** processor. When the Create Connection window appears, verify **ValidateTransitObservations** checkbox is checked, if not check it. Click **Add**.

![RouteOnAttribute_to_InvokeHTTP](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/RouteOnAttribute_to_InvokeHTTP.png)

**Figure 2:** Connect **ValidateNextBusData** to **InvokeHTTP**

2\. Open **InvokeHTTP** configure properties tab and add the property listed in **Table 2**.

**Table 2:** Update **InvokeHTTP** Properties Tab

| Property  | Value  |
|:---|:---|
| `Remote URL`  | `https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=${Latitude},${Longitude}&radius=500&type=neighborhood&key=AIzaSyDY3asGAq-ArtPl6J2v7kcO_YSRYrjTFug` |

**Remote URL** connects to the HTTP URL we created using Google Places API and feeds that data into the dataflow. Notice we use two NiFi expressions for location parameter. This is because those two values change as new FlowFiles pass through this processor.

3\. Navigate to the **Settings** tab, change the name from InvokeHTTP to `GoogleNearbySearchAPI`. Under Auto terminate relationships check the **Failure**, **No Retry**, **Original** and **Retry** checkboxes. Click **Apply** button.

### Step 6: Add EvaluateJsonPath to Extract GeoEnriched Transit Data

1\. Add the **EvaluateJsonPath** processor onto the NiFi graph. Connect **GoogleNearbySearchAPI** to **EvaluateJsonPath** processor. When the Create Connection window appears, select **Response** checkbox. Click Add.

![InvokeHTTP_to_EvaluateJsonPath](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/InvokeHTTP_to_EvaluateJsonPath.png)

**Figure 3:** Connect **GoogleNearbySearchAPI** to **EvaluateJsonPath**

2\. Open EvaluateJsonPath configure properties tab and update the original properties with the properties listed in **Table 3**. Note: add `city` and `neighborhoods_nearby` property by clicking the **( + )** button, then insert their values into the properties tab.

**Table 3:** Update **EvaluateJsonPath** Properties Tab

| Property  | Value  |
|:---|:---|
| `Destination`  | `flowfile-attribute`  |
| `Return Type`  | `json`  |
| `city`  | `$.results[0].vicinity`  |
| `neighborhoods_nearby`  | `$.results[*].name`  |

- **Destination** result from JSON Path Evaluation stored in FlowFile attributes.
- **2 user-defined attributes** each hold a value that is used in the NiFi Expression language filtering condition in the next processor.

3\. Navigate to the **Settings** tab, change the name from EvaluateJsonPath to `ExtractGeoEnrichedData`. Under Auto terminate relationships check the **unmatched** and **failure** checkboxes. Click **Apply** button.

### Step 7: Add RouteOnAttribute to Validate Google Places Data

1\. Add the **RouteOnAttribute** processor onto the NiFi graph. Connect **ExtractGeoEnrichedData** to **RouteOnAttribute** processor. When the Create Connection window appears, select **matched** checkbox. Click Add.

![EvaluateJsonPath_to_RouteOnAttribute](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/EvaluateJsonPath_to_RouteOnAttribute.png)

**Figure 4:** Connect **ExtractGeoEnrichedData** to **RouteOnAttribute**

2\. Open RouteOnAttribute configure properties tab and click on **New property** button to add `ValidateGooglePlacesData` to property name and insert its NiFi expression value listed in **Table 4**.

**Table 4:** Update **ValidateGooglePlacesData** Properties Tab

| Property  | Value  |
|:---|:---|
| Routing Strategy  | Route to 'matched' if all match |
| `IfCityIsNotEmpty`  | `${city:isEmpty():not()}`  |
| `IfNeighborhoodNotEmpty`  | `${neighborhoods_nearby:isEmpty():not()}`  |

~~~java
//Note: If after completing step 2 your processor shows a triangle with an exclamation mark stating that the ValidateGooglePLacesData is invalid, copy and paste into the value section of the property tab.

${city:isEmpty():not():and(
	${neighborhoods_nearby:isEmpty():not()})}
~~~

**ValidateGooglePlacesData** uses the FlowFile Attribute values obtained from JSON Path Expressions to filter out any FlowFiles that have at least one empty Attribute value. Else the FlowFiles are passed to the remaining processors.

3\. Navigate to the **Settings** tab, change the name from RouteOnAttribute to `ValidateGooglePlacesData`. Under Auto terminate relationships check the **unmatched** checkbox. Click **Apply** button.

### Step 8: Add an Output Port to Route Data Outside this PG

1\. Add the **Output Port** ![output_port](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/output_port.png) component onto the NiFi canvas. Name it `SendGeoEnrichedTransitEvents`.

2\. Connect **ValidateGooglePlacesData** to **SendGeoEnrichedTransitEvents** output port. When the Create Connection window appears ensure that the **For Relationship** field is **Matched**. Finally, click Add.

![ParseTransitEvents_dataflow_pg](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/RouteOnAttribute_to_SendGeoEnrichedTransitEvents.png)

**Figure 5:** Connect **ValidateGooglePlacesData** to **SendGeoEnrichedTransitEvents**

### Step 9: Connect ParseTransitEvents to ValidateGeoEnrichedTransitData

1\. Re-enter the `NiFi Flow` breadcrumb to exit **ValidateGeoEnrichedTransitData**.

2\. Connect **ParseTransitEvents** to **ValidateGeoEnrichedTransitData** process group. When the Create Connection window appears, verify **From Output = ParsedTransitEvents** and connects to **To Input = IngestParsedTransitEvents**. Click **Add**.

![SimulateXmlTransitEvents_to_ParseTransitEvents](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/ValidateGeoEnrichedTransitData_connect_rest_flow.png)

**Figure 6:** Connect **ParseTransitEvents** to **ValidateGeoEnrichedTransitData** Process Group

### Step 10: Run the DataFlow

1\. Hold **Shift** and drag your mouse across the entire dataflow. You should see the dataflow is highlighted.

![selected_entire_dataflow](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/selected_entire_dataflow.jpg)

**Figure 7:** Selected the entire dataflow

2\. In the **Operate** panel, you should see the header says **Multiple components are selected**. Press the **Start** button.

![start_dataflow](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/start_dataflow.jpg)

**Figure 8:** Started the entire dataflow

### Step 11: Verify GeoEnriched Data Routed by ValidateGooglePlacesData is Valid

Inside the **ValidateGeoEnrichedTransitData** process group, we will inspect **ValidateGooglePlacesData** processor. We will look at the flowfiles that pass through this processor and analyze their attributes. We need to make sure that each flowfile's attribute keys **city** and **neighborhoods_nearby** do not have an empty corresponding value. FlowFile attributes are in **key/value** format.

1\. Jump into **ValidateGeoEnrichedTransitData** process group.

2\. Right click on the **ValidateGooglePlacesData** connector downstream from **ValidateGooglePlacesData** processor, select **List queue** to see provenance events.

![routenearbyneighborhoods_listqueue](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/ValidateGooglePlacesData_listqueue.png)

**Figure 9:** List Queue for Connector between **ValidateGooglePlacesData** and **SendGeoEnrichedTransitEvents**

3\. View any event by selecting the view provenance event icon ![i_symbol_nifi](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/i_symbol_nifi_lab1.png)

4\. Click on the **Attributes** tab. As long as you see that **city** and **neighborhoods_nearby** have values, you have verified the **ValidateGooglePlacesData** processor is successfully routing flowfiles based on this condition.

![verify_evaluateXPath_extracts_data](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/verify_data_routed_by_routeonattribute.png)

**Figure 10:** Data Provenance for FlowFile attribute (key/value): **city** contains **San Francisco**.

![verify_data_routed_by_routeonattribute_p2](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/verify_data_routed_by_routeonattribute_p2.png)

**Figure 11:** Data Provenance for FlowFile attribute (key/value): **neighborhoods_nearby** contains **["Saint Francis Wood","West Portal"]**

## Approach 2: Import Enriched NiFi Flow Via Places API

**Warning 1: If you Imported the Previous DataFlow, Read these steps first**

1\. In your process group, if there are any **queues** left with data, remove the data. Right click the queue, select **Empty queue**, click **EMPTY**.

2\. Navigate back to the **NiFi Flow** breadcrumb level to delete your flow.

To delete your previous flow, hold **command** or **ctrl** and press **A** to highlight your dataflow, then press **delete** or **backspace**.

**Import the New DataFlow Template**

3\. Download the [tutorial-5-ValidateGeoEnrichedTransitData.xml](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/template/tutorial-5-ValidateGeoEnrichedTransitData.xml) template file. Then upload the template file into NiFi.

4\. Use the template icon ![nifi_template_icon](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/nifi_template_icon.png) located in the **Operate** Palette.

5\. **Browse**, find the template file, click **Open** and hit **Upload**.

6\. From the **Components Toolbar**, drag the template icon ![nifi_template_icon](assets/tutorial-4-build-nifi-process-group-to-parse-transit-events/add_nifi_template.png) onto the graph and select the **tutorial-5-ValidateGeoEnrichedTransitData.xml** template file.

**Warning 2: If you didn't Obtain Google API Key, then follow the steps**

7\. Refer to [Step 1](#step-1-obtain-api-key-for-nifis-invokehttp-processor) in **Approach 1** to obtain the Google API key and set up **Google Places API: HTTP URL**.

8\. Replace the **InvokeHTTP** processor's **Remote URL** property value with the new **Google Places API: HTTP URL** value.

9\. Hit the **start** button ![start_button_nifi_iot](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/start_button_nifi_iot.png). to activate the dataflow.

![complete_dataflow_lab2_geoEnrich](assets/tutorial-5-build-a-nifi-process-group-to-validate-the-geoenriched-data/ValidateGeoEnrichedTransitData_pg.png)

**Figure 12:** **tutorial-5-ValidateGeoEnrichedTransitData.xml** template includes a NiFi Flow that pulls in San Francisco Muni Transit Events from the XML Simulator, parses through the data to extract key values and stores the transit observations as a JSON file.

Overview of the Process Groups and their Processors:

- **SimulateXmlTransitEvents (Process Group)**
  - **GetFile** fetches the vehicle location simulator data for files in a directory.
  - **UnpackContent** decompresses the contents of FlowFiles from the traffic simulator zip file.
  - **ControlRate** controls the rate at which FlowFiles are transferred to follow-on processors enabling traffic simulation.
  - **UpdateAttribute** renames every FlowFile to give them unique names
  - **PutFile** stores data to local file system
  - **Output Port** makes the connection for the process group to connect to other components (process groups, processors, etc)

- **ParseTransitEvents (Process Group)**
  - **Input Port** ingests data from SimulateXmlTransitEvents Process Group
  - **ExtractTimestamp** extracts the timestamp of the last update for vehicle location data returned from each FlowFile.
  - **SplitXML** splits the parent's child elements into separate FlowFiles. Since vehicle is a child element in our xml file, each new vehicle element is stored separately.
  - **ExtractTransitObservations** extracts attributes: vehicle id, direction, latitude, longitude and speed from vehicle element in each FlowFile.
  - **Output Port** outputs data with the new FlowFile attribute (key/values) to the rest of the flow

- **ValidateGooglePlacesData (Process Group)**
  - **Input Port** ingests data from ParseTransitEvents Process Group
  - **ValidateNextBusData** checks the NextBus Simulator data by routing FlowFiles only if their attributes contain transit observation data (Direction_of_Travel, Last_Time, Latitude, Longitude, Vehicle_ID, Vehicle_Speed)
  - **InvokeHTTP** sends a rest call to Google Places API to pull in geo enriched data for transit location
  - **EvaluateJSONPath** parses the flowfile content for city and neighborhoods_nearby
  - **ValidateGooglePlacesData** checks the new Google Places data by routing FlowFiles only if their attributes contain geo enriched data (city, neighborhoods_nearby)
  - **Output Port** outputs data with nonempty FlowFile attributes (key/values) to the rest of the flow

## Summary

Congratulations! For the Geo Enrichment section of the dataflow, you learned to use **InvokeHTTP** to access geographic location of nearby places with Google Places Search API. You learned to add NiFi expression variables into InvokeHTTP property RemoteURL, so that the values for **latitude** and **longitude** constantly change in the URL when new FlowFiles pass through this processor. You learned to use **EvaluateJsonPath** similar to EvaluateXPath, except JSON Expression is used to extract JSON elements **(neighborhoods_nearby & city)** from a JSON structure. Now you know how to incorporate external API's into NiFi further enhance the dataflow.

## Further Reading

- [Google Places API](https://developers.google.com/places/)
- [HTTP Protocol Overview](http://code.tutsplus.com/tutorials/http-the-protocol-every-web-developer-must-know-part-1--net-31177)
- [JSON Path Expressions](http://goessner.net/articles/JsonPath/index.html#e2)
- [JSON Path Online Evaluator](http://jsonpath.com/)
- [NiFi Expression Language Guide](https://nifi.apache.org/docs/nifi-docs/html/expression-language-guide.html)



---
title: Build a NiFi Process Group to Store Data As JSON
---

# 6.Build a NiFi Process Group to Store Data As JSON

## Introduction

You will build a new process group called **StoreTransitEventsAsJSONToDisk** to take the FlowFile attributes, write them to a new FlowFile's content as JSON representation. You will then store these FlowFiles to the local filesystem.

## Prerequisites

- Completed the prior tutorials within this tutorial series

## Outline

- [Approach 1: Manually Build StoreTransitEventsAsJSONToDisk Process Group (PG)](#approach-1-manually-build-storetransiteventsasjsontodisk-process-group-pg)
- [Step 1: Create Process Group and Label For It](#step-1-create-process-group-and-label-for-it)
- [Step 2: Add an Input Port to Ingest Data Into this PG](#step-2-add-an-input-port-to-ingest-data-into-this-pg)
- [Step 3: Add AttributesToJSON to Represent FlowFile Attributes As JSON](#step-3-add-attributestojson-to-represent-flowfile-attributes-as-json)
- [Step 4: Add MergeContent to Combine Multiple FlowFiles Together](#step-4-add-mergecontent-to-combine-multiple-flowfiles-together)
- [Step 5: Add UpdateAttribute to Make Each FlowFile Name Unique](#step-5-add-updateattribute-to-make-each-flowfile-name-unique)
- [Step 6: Add PutFile to Store Data to Local File System](#step-6-add-putfile-to-store-data-to-local-file-system)
- [Step 7: Connect ValidateGeoEnrichedTransitData to StoreDataAsJSONToDisk](#step-7-connect-validategeoenrichedtransitdata-to-storedataasjsontodisk)
- [Step 8: Run the NiFi DataFlow](#step-8-run-the-nifi-dataflow)
- [Step 9: Check Data By NiFi's Data Provenance](#step-9-check-data-by-nifis-data-provenance)
- [Approach 2: Import StoreDataAsJSONToDisk Process Group](#approach-2-import-storedataasjsontodisk-process-group)
- [Summary](#summary)

If you prefer to build the dataflow manually step-by-step, continue on to [Approach 1](#approach-1-manually-build-storetransiteventsasjsontodisk-process-group-pg). Else if you want to see the NiFi flow in action within minutes, refer to [Approach 2](#approach-2-import-storedataasjsontodisk-process-group).

### Approach 1: Manually Build StoreTransitEventsAsJSONToDisk Process Group (PG)

### Step 1: Create Process Group and Label For It

1\. Add a Label ![label_icon](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/label_icon.png) onto the NiFi canvas for the new process group. Right click, **Change color** to **Green**.

2\. Right click, select configure and name it `Take the FlowFile Attributes, Represent as JSON and Store Data to Local FileSystem`.

![label_for_ValidateGeoEnrichedTransitData](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/label_for_StoreTransitEventsAsJSONToDisk.png)

3\. Add a new Process Group onto the NiFi canvas and name it `StoreDataAsJSONToDisk`

![ValidateGeoEnrichedTransitData](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/StoreDataAsJSONToDisk.png)

4\. Double click on the new process group to enter it.

![ParseTransitEvents](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/breadcrumb_StoreDataAsJSONToDisk.png)

### Step 2: Add an Input Port to Ingest Data Into this PG

1\. Add the **Input Port** ![input_port](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/input_port.png) component onto the NiFi canvas. Name it `IngestGeoEnrichedEvents`.

![IngestGeoEnrichedEvents](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/IngestGeoEnrichedEvents.png)

### Step 3: Add AttributesToJSON to Represent FlowFile Attributes As JSON

1\. Add the **AttributesToJSON** processor onto the NiFi canvas.

2\. Connect **IngestGeoEnrichedEvents** input port to **AttributesToJSON** processor.

![IngestGeoEnrichedEvents_to_AttributesToJSON](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/IngestGeoEnrichedEvents_to_AttributesToJSON.png)

**Figure 1:** Connect **IngestGeoEnrichedEvents** input port to **AttributesToJSON** processor

3\. Open the processor configuration **properties** tab. Add the properties listed in **Table 1** and if their original properties already have values, update them.

**Table 1:** Update **AttributesToJSON** Properties Tab

| Property  | Value  |
|:---|---:|
| `Attributes List`  | `Vehicle_ID, Direction_of_Travel, Latitude, Longitude, Vehicle_Speed, Last_Time`  |
| `Destination`  | flowfile-content  |

- **Attributes List** takes FlowFile attribute parameters and presents them in JSON format
- **Destination** stores the output as content in the FlowFile

4\. Open the processor config **Settings** tab, under Auto terminate relationships, check the **failure** checkbox. Click **Apply**.

### Step 4: Add MergeContent to Combine Multiple FlowFiles Together

1\. Add the **MergeContent** processor onto the NiFi canvas.

2\. Connect **AttributesToJSON** to **MergeContent** processor. When the Create Connection window appears, verify **success** checkbox is checked, else check it. Click **Add**.

![AttributesToJSON_to_MergeContent](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/AttributesToJSON_to_MergeContent.png)

**Figure 2:** Connect **AttributesToJSON** to **MergeContent** processor

3\. Open the processor configuration **properties** tab. Add the properties listed in **Table 2** and if their original properties already have values, update them.

**Table 2:** Update **MergeContent** Properties Tab

| Property  | Value  |
|:---|---:|
| `Minimum Number of Entries`  | `10`  |
| `Maximum Number of Entries`  | `15`  |
| `Delimiter Strategy`  | Text  |
| `Header`  | `[`  |
| `Footer`  | `]`  |
| `Demarcator` | `,` {now-press-shift-enter} |

- **Minimum Number of Entries** takes in at least specified amount of FlowFiles, then merges them into 1 FlowFiles
- **Maximum Number of Entries** takes in no more than specified amount of FlowFiles, then merges them
- **Delimiter Strategy** specifies that Header, Footer and Demarcator set the formatting conditions for text in the file
- **Header** inserts specified value at the beginning of the file
- **Footer** inserts specified value at the end of the file
- **Demarcator** inserts specified value(s) at the end of every line in the file

4\. Open the processor config **Settings** tab, under Auto terminate relationships, check the **failure** and **original** checkboxes. Click **Apply**.

### Step 5: Add UpdateAttribute to Make Each FlowFile Name Unique

1\. Add the **UpdateAttribute** processor onto the NiFi canvas.

2\. Connect **MergeContent** to **UpdateAttribute** processor. When the Create Connection window appears, verify **merged** checkbox is checked, if not check it. Click **Add**.

![MergeContent_to_UpdateAttribute](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/MergeContent_to_UpdateAttribute.png)

**Figure 3:** Connect **MergeContent** to **UpdateAttribute** processor

3\. Open the processor configuration properties tab. Add the properties listed in **Table 3** and if their original properties already have values, update them.

**Table 3:** Add **UpdateAttribute** Properties Tab

| Property  | Value  |
|:---|---:|
| `filename`  | `${UUID()}`  |

- **filename** updates each FlowFile with a unique identifier

### Step 6: Add PutFile to Store Data to Local File System

1\. Add the **PutFile** processor onto the NiFi canvas.

2\. Connect **UpdateAttribute** to **PutFile** processor. When the Create Connection window appears, verify **success** checkbox is checked, else check it. Click Add.

![putFile_config_property_tab_window](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/UpdateAttribute_to_PutFile.png)

**Figure 4:** Connect **UpdateAttribute** to **PutFile** processor

3\. Open the processor configuration **properties** tab. Add the property listed in **Table 4** and if their original property already has a value, update it.

**Table 4:** Update **PutFile** Property Value

| Property  | Value  |
|:---|---:|
| `Directory`  | `/sandbox/tutorial-files/640/nifi/output/observe-transit-data`  |

4\. Open the processor config **Settings** tab, under Auto terminate relationships, check the **failure** and **success** checkboxes. Click **Apply**.

### Step 7: Connect ValidateGeoEnrichedTransitData to StoreDataAsJSONToDisk

1\. Re-enter the `NiFi Flow` breadcrumb to exit **StoreDataAsJSONToDisk** PG.

2\. Connect **ValidateGeoEnrichedTransitData** to **StoreDataAsJSONToDisk** process group. When the Create Connection window appears, verify **From Output = SendGeoEnrichedTranistEvents** and connects to **To Input = IngestGeoEnrichedEvents**. Click **Add**.

![StoreDataAsJSONToDisk_connect_rest_flow](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/StoreDataAsJSONToDisk_connect_rest_flow.png)

**Figure 5:** Connect **ValidateGeoEnrichedTransitData** to **StoreDataAsJSONToDisk** PG

### Step 8: Run the NiFi DataFlow

1\. The processors are valid since the warning symbols disappeared. Notice that the process groups have a highlighted red stop symbol ![stop_symbol_nifi_iot](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/stop_symbol_nifi.png) as shown back in **Figure 5**. To select all process groups, hold down the **shift-key** and drag your mouse across the entire data flow.

2\. Now that all processors are selected, go to the actions toolbar in the left window labeled **Operate** and click the start button ![start_button_nifi_iot](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/start_button_nifi.png). Your process groups should change to a green highlighted start symbol. Your screen should look like the following:

![start_nifi_flow](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/start_nifi_flow.png)

**Figure 6:** Start **StoreDataAsJSONToDisk** PG

3\. To quickly see what the processors are doing and the information on their faces, right click on the canvas, click the **refresh status** button ![refresh_nifi](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/refresh_nifi.png)

### Step 9: Check Data By NiFi's Data Provenance

1\. Before we go see **PutFile**, we must double-click on **StoreDataAsJSONToDisk**. Right click on the PutFile processor. Select `View Data Provenance`.

NiFi searches for provenance events. The window will load with events, select any event. An event is a FlowFile that passes through a processor and the data that is viewable at that particular time.

2\. Select any event by pressing on the view provenance event symbol ![i_symbol_nifi](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/i_symbol_nifi.png).

![provenance_event_window](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/provenance_event_window.png)

**Figure 7:** NiFi Data Provenance Window

3\. Once you select the event, a Provenance Event Dialog Window will appear. It contains Details, Attributes and Content regarding the particular event. Take a few minutes to view each tab. Let's navigate to the `Content` tab to view the data generated from the FlowFile. NiFi gives the user the option to download or view the content of the event. Click on the **View** button.

![provenance_content_tab](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/provenance_content_tab.png)

**Figure 8:** Provenance Event Window

4\. NiFi gives the user the option view the data in multiple formats. We will view it in original format.

![event_content_view_window](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/event_content_view_window.png)

**Figure 9:** View FlowFile JSON Content

Did you receive the data you expected?

### Approach 2: Import StoreDataAsJSONToDisk Process Group

**Warning 1: If you Imported the Previous DataFlow, Read these steps first**

1\. In your process group, if there are any **queues** left with data, remove the data. Right click the queue, select **Empty queue**, click **EMPTY**.

2\. Navigate back to the **NiFi Flow** breadcrumb level to delete your flow.

To delete your previous flow, hold **command** or **ctrl** and press **A** to highlight your dataflow, then press **delete** or **backspace**.

**Import the New DataFlow Template**

3\. Download the [tutorial-6-StoreDataAsJSONToDisk.xml](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/template/tutorial-6-StoreDataAsJSONToDisk.xml) template file.

4\. Use the **upload template** icon ![upload_nifi_template](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/nifi_template_icon.png) located in the Operate Palette.

5\. **Browse**, find the template file, click **Open** and hit **Upload**.

6\. From the **Components Toolbar**, drag the **add template** icon ![add_nifi_template](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/add_nifi_template.png) onto the graph and select the **tutorial-6-StoreDataAsJSONToDisk.xml** template file.

**Warning 2: If you didn't Obtain Google API Key, then follow the steps**

7\. Refer to [Step 1](#step-1-obtain-api-key-for-nifis-invokehttp-processor) in **Approach 1** from the **Validate the GeoEnriched Data Tutorial** to obtain the Google API key and set up **Google Places API: HTTP URL**.

8\. Once you have your own Google Places API Key, go into the ValidateGeoEnrichedTransitData process group, replace the **InvokeHTTP** processor's **Remote URL** property value with the new **Google Places API: HTTP URL** value.

9\. Hit the **start** button ![start_button_nifi](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/start_button_nifi.png) to activate the dataflow.

![start_nifi_flow](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/start_nifi_flow.png)

**Figure 10:** **tutorial-6-StoreDataAsJSONToDisk.xml** template includes a NiFi Flow that pulls in San Francisco Muni Transit Events from the XML Simulator, parses through the data to extract key values and stores the transit observations as a JSON file.

Overview of the Process Groups and their Processors:

- **SimulateXmlTransitEvents (Process Group)**
  - **GetFile** fetches the vehicle location simulator data for files in a directory.
  - **UnpackContent** decompresses the contents of FlowFiles from the traffic simulator zip file.
  - **ControlRate** controls the rate at which FlowFiles are transferred to follow-on processors enabling traffic simulation.
  - **UpdateAttribute** renames every FlowFile to give them unique names
  - **PutFile** stores data to local file system
  - **Output Port** makes the connection for the process group to connect to other components (process groups, processors, etc)


- **ParseTransitEvents (Process Group)**
  - **Input Port** ingests data from SimulateXmlTransitEvents Process Group
  - **ExtractTimestamp** extracts the timestamp of the last update for vehicle location data returned from each FlowFile.
  - **SplitXML** splits the parent's child elements into separate FlowFiles. Since vehicle is a child element in our xml file, each new vehicle element is stored separately.
  - **ExtractTransitObservations** extracts attributes: vehicle id, direction, latitude, longitude and speed from vehicle element in each FlowFile.
  - **Output Port** outputs data with the new FlowFile attribute (key/values) to the rest of the flow


- **ValidateGooglePlacesData (Process Group)**
  - **Input Port** ingests data from ParseTransitEvents Process Group
  - **ValidateNextBusData** checks the NextBus Simulator data by routing FlowFiles only if their attributes contain transit observation data (Direction_of_Travel, Last_Time, Latitude, Longitude, Vehicle_ID, Vehicle_Speed)
  - **InvokeHTTP** sends a rest call to Google Places API to pull in geo enriched data for transit location
  - **EvaluateJSONPath** parses the flowfile content for city and neighborhoods_nearby
  - **ValidateGooglePlacesData** checks the new Google Places data by routing FlowFiles only if their attributes contain geo enriched data (city, neighborhoods_nearby)
  - **Output Port** outputs data with nonempty FlowFile attributes (key/values) to the rest of the flow


- **StoreTransitEventsAsJSONToDisk (Process Group)**
  - **Input Port** ingests data from ValidateGooglePlacesData Process Group
  - **AttributesToJSON** generates a JSON representation of the attributes extracted from the FlowFiles and converts XML to JSON format this less attributes.
  - **MergeContent** merges a group of JSON FlowFiles together based on a number of FlowFiles and packages them into a single FlowFile.
  - **UpdateAttribute** updates the attribute name for each FlowFile.
  - **PutFile** writes the contents of the FlowFile to a desired directory on the local filesystem.

Refer to [NiFi's Documentation](https://nifi.apache.org/docs.html) to learn more about each processor described above.

## Summary

Congratulations! For the **StoreTransitEventsAsJSONToDisk** Process Group, you learned to take the FlowFile attribute (key/value) pairs and represent them as a JSON format. You then used **MergeContent** to combine multiple FlowFiles together to create a larger FlowFile with multiple records. **UpdateAttribute** was used to ensure no FlowFiles would have duplicate names. **PutFile** was added to the flow to store the JSON data into the local file system.



---
title: Integrate NextBus API To Pull In Transit Live Feed
---

# 7.Integrate NextBus API To Pull In Transit Live Feed

## Introduction

You will learn to perform **rest calls** against the **NextBus API** to retrieve transit data. You will replace the SimulateXmlTransitEvents Process Group data seed with a new processor that pulls in live stream data from **San Francisco Muni Agency** on route **OceanView** into the NiFi DataFlow.

## Prerequisites

- Completed the prior tutorials within this tutorial series

## Outline

- [Approach 1: Manually Integrate NextBus API into NiFi Flow](#approach-1-manually-integrate-nextbus-api-into-nifi-flow)
- [NextBus Live Feed API Basics](#nextbus-live-feed-api-basics)
- [Step 1: Add GetHTTP to Make Rest Calls and Ingest Data via NextBus API](#step-1-add-gethttp-to-make-rest-calls-and-ingest-data-via-nextbus-api)
- [Step 2: Modify PutFile in StoreDataAsJSONToDisk Process Group](#step-2-modify-putfile-in-storedataasjsontodisk-process-group)
- [Step 3: Run the NiFi DataFlow](#step-3-run-the-nifi-dataflow)
- [Step 4: Explore Back Pressure for NextBus API Rest Calls](#step-4-explore-back-pressure-for-nextbus-api-rest-calls)
- [Approach 2: Import NextBusAPIIntegration NiFi Flow](#approach-2-import-nextbusapiintegration-nifi-flow)
- [Summary](#summary)
- [Further Reading](#further-reading)

If you prefer to build the dataflow manually step-by-step, continue on to [Approach 1]((#approach-1-manually-integrate-nextbus-api-into-nifi-flow)). Else if you want to see the NiFi flow in action within minutes, refer to [Approach 2](#approach-2-import-nextbusapiintegration-nifi-flow).

You will need to understand NextBus API, so that it will be easier to incorporate this API's data into the NiFi flow, which will be built in **Approach 1**.

## Approach 1: Manually Integrate NextBus API into NiFi Flow

## NextBus Live Feed API Basics

NextBus Live Feed provides the public with live information regarding passenger information, such as vehicle location information, prediction times on transit vehicles, routes of vehicles and different agencies (San Francisco Muni, Unitrans City of Davis, etc). We will learn to use NextBus's API to access the XML Live Feed Data and create an URL. In this URL we will specify parameters in a query string. The parameters for the tutorial will include the vehicle location, agency, route and time.

After viewing the Live Feed Documentation, we created the following URL for the GetHTTP processor:

~~~text
http://webservices.nextbus.com/service/publicXMLFeed?command=vehicleLocations&a=sf-muni&r=M&t=0
~~~

Let’s break apart the parameters, so we can better understand how to create custom URLs. There are 4 parameters:

- commands: command = vehicleLocations
- agency: a=sf-muni
- route: r=M
- time: t=0

Refer to [NextBus’s Live Feed Documentation](https://www.nextbus.com/xmlFeedDocs/NextBusXMLFeed.pdf) to learn more about each parameter.

### Step 1: Add GetHTTP to Make Rest Calls and Ingest Data via NextBus API

You will replace the **SimulateXmlTransitEvents** Process Group with **GetHTTP** processor.

1\. Check the queue between **SimulateXmlTransitEvents** and **ParseTransitEvents**, if the queue has FlowFiles, then empty the queue. Right click on the queue, click **Empty queue**.

![empty_queue](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/empty_queue.png)

2\. Right click on the queue again. Then press **Delete**.

3\. Enter the **SimulateXmlTransitEvents**, then verify if other queues have FlowFiles and empty those queues. Once all queues are empty, step out of the Process Group.

![simulate_nb_api_pg_empty_queues](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/simulate_nb_api_pg_empty_queues.png)

4\. Right click on **SimulateXmlTransitEvents** Process Group, select **Delete** to remove it. Add **GetHTTP** processor to replace **SimulateXmlTransitEvents**.

5\. Connect **GetHTTP** to **ParseTransitEvents**. Verify **success** is checked under For Relationships. Click **ADD**.

![gethttp_to_parsetransitevents](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/gethttp_to_parsetransitevents.png)

6\. Replace the text in the label above **GetHTTP** with the following: `Ingest Real-Time Transit Data via NextBus API`

![label_gethttp_updated](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/label_gethttp_updated.png)

2\. Open **GetHTTP** Config Property Tab window. We will need to copy and paste Nextbus XML Live Feed URL into the property value. Add the property listed in **Table 1**.

**Table 1:** Update GetHTTP Properties Tab

| Property  | Value  |
|:---|:---|
| `URL`  | `http://webservices.nextbus.com/service/publicXMLFeed?command=vehicleLocations&a=sf-muni&r=M&t=0` |
| `Filename`  | `live_transit_data_${now():format("HHmmssSSS")}.xml` |

3\. Now that each property is updated. Navigate to the **Scheduling tab** and change the **Run Schedule** from 0 sec to `6 sec`, so that the processor executes a task every 6 seconds.

4\. Open the processor config **Settings** tab, change the processor's Name from GetHTTP to `IngestNextBusXMLData`. Click **Apply** button.

### Step 2: Modify PutFile in StoreDataAsJSONToDisk Process Group

You will change the directory PutFile writes data to since data is coming in from the NextBus API live feed instead of the Simulator.

1\. Open **PutFile** Configure **Properties Tab**. Change the Directory property value from the previous value to the value shown in **Table 2**:

**Table 2:** Update PutFile Properties Tab

| Property  | Value  |
|:---|:---|
| `Directory`  | `/sandbox/tutorial-files/640/nifi/output/live_transit_data` |

**Directory** is changed to a new location for the real-time data coming in from NextBus live stream.

2\. Click **Apply**. Then go back to the **NiFi Flow** breadcrumb.

### Step 3: Run the NiFi DataFlow

Now that we added NextBus San Francisco Muni Live Stream Ingestion to our dataflow , let's run the dataflow and verify if we receive the expected results in our output directory.

1\. Go to the actions toolbar and click the start button ![start_button_nifi](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/start_button_nifi.png). Your screen should look like the following:

![live_stream_ingestion_flow](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/live_stream_ingestion_flow.png)

2\. Let's verify the data sent to the output directory is correct. Open the Data Provenance for the **PutFile** processor located in **StoreDataAsJSONToDisk** PG. Select **View Data Provenance** and **View** its content.

Did you receive neighborhoods similar to the image below?

![data_provenance_neighborhoods_nearby](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/data_provenance_neighborhoods_nearby.png)

### Step 4: Explore Back Pressure for NextBus API Rest Calls

Back pressure allows us to specify the amount of data allowed to exist in the queue before the component that is the source of connection is no longer scheduled to run.

For example, let's see what happens when **IngestNextBusXMLData** processor runs at the fastest possible rate it can execute tasks.

1\. Configure **IngestNextBusXMLData** under **Scheduling** tab, change **Run Schedule = 0**.

![update_gethttp_scheduling](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/update_gethttp_scheduling.jpg)

What impact does this place on the remaining dataflow? When you inspect the process groups, you'll notice some of the queues are being filled with a lot of data and the processor connected to it connect keep up with processing that data.

2\. Jump into **ParseTransitEvents** process group, right between **ExtractTimestamp** and **SplitXml**, you will see the queue is becoming very large.

![back_pressure_percentage](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/back_pressure_percentage.jpg)

> Note: We are able to see a percentage indicator bar on the left for the **Back Pressure Object Threshold** is **Green**, which means the queue is **0 - 60%** full.

![back_pressure_percentage_indicator](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/back_pressure_percentage_indicator.jpg)

By default the queue can be filled with 10,000 objects or FlowFiles.

3\. Right click on **matched** queue, click **View configuration**.

![view_queue_configuration](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/view_queue_configuration.jpg)

What happens when the queue reaches that threshold? Back pressure will be applied and the processor sourcing the data to the **matched** queue will deactivate.

![back_pressure_object_threshold](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/back_pressure_object_threshold.jpg)

- the threshold of flowfiles that can be in the queue is 10,000

There is another configuration element for Back Pressure called **Back Pessure Data Size Threshold**. This configuration specifies the max amount of data (in size) that should be queued before back pressure is triggered. By default the max data size allowed in the queue before back pressure is applied is **1GB**.

> Note: Depending on your application, you may need to modify these values, so that more or less data is filled into a queue.

## Approach 2: Import NextBusAPIIntegration NiFi Flow

**Warning: If you Imported the Previous DataFlow, Read these steps first**

1\. In your process group, if there are any **queues** left with data, remove the data. Right click the queue, select **Empty queue**, click **EMPTY**.

2\. Navigate back to the **NiFi Flow** breadcrumb level to delete your flow.

To delete your previous flow, hold **command** or **ctrl** and press **A** to highlight your dataflow, then press **delete** or **backspace**.

**Import the New DataFlow Template**

3\. Download the [tutorial-7-ingest-live-nextbus-api.xml](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/template/tutorial-7-ingest-live-nextbus-api.xml) template file. Then import the template file into NiFi.

4\. Use the **upload template** icon ![upload_nifi_template](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/nifi_template_icon.png) located in the Operate Palette.

5\. **Browse**, find the template file, click **Open** and hit **Upload**.

6\. From the **Components Toolbar**, drag the **add template** icon ![add_nifi_template](assets/tutorial-6-build-a-nifi-process-group-to-store-data-as-json/add_nifi_template.png) onto the graph and select the **tutorial-7-ingest-live-nextbus-api.xml** template file.

**Warning 2: If you didn't Obtain Google API Key, then follow the steps**

7\. Refer to [Step 1](#step-1-obtain-api-key-for-nifis-invokehttp-processor) in **Approach 1** from the **Validate the GeoEnriched Data Tutorial** to obtain the Google API key and set up **Google Places API: HTTP URL**.

8\. Once you have your own Google Places API Key, go into the ValidateGeoEnrichedTransitData process group, replace the **InvokeHTTP** processor's **Remote URL** property value with the new **Google Places API: HTTP URL** value.

9\. Hit the **start** button ![start_button_nifi](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/start_button_nifi.png) to activate the dataflow.

![live_stream_ingestion_flow](assets/tutorial-7-integrate-nextbus-api-to-pull-in-transit-live-feed/live_stream_ingestion_flow.png)

Overview of the NiFi Flow:

- **GetHTTP** ingests real-time transit observation data from NextBus API


- **ParseTransitEvents (Process Group)**
  - **Input Port** ingests data from SimulateXmlTransitEvents Process Group
  - **ExtractTimestamp** extracts the timestamp of the last update for vehicle location data returned from each FlowFile.
  - **SplitXML** splits the parent's child elements into separate FlowFiles. Since vehicle is a child element in our xml file, each new vehicle element is stored separately.
  - **ExtractTransitObservations** extracts attributes: vehicle id, direction, latitude, longitude and speed from vehicle element in each FlowFile.
  - **Output Port** outputs data with the new FlowFile attribute (key/values) to the rest of the flow


- **ValidateGooglePlacesData (Process Group)**
  - **Input Port** ingests data from ParseTransitEvents Process Group
  - **ValidateNextBusData** checks the NextBus Simulator data by routing FlowFiles only if their attributes contain transit observation data (Direction_of_Travel, Last_Time, Latitude, Longitude, Vehicle_ID, Vehicle_Speed)
  - **InvokeHTTP** sends a rest call to Google Places API to pull in geo enriched data for transit location
  - **EvaluateJSONPath** parses the flowfile content for city and neighborhoods_nearby
  - **ValidateGooglePlacesData** checks the new Google Places data by routing FlowFiles only if their attributes contain geo enriched data (city, neighborhoods_nearby)
  - **Output Port** outputs data with nonempty FlowFile attributes (key/values) to the rest of the flow


- **StoreTransitEventsAsJSONToDisk (Process Group)**
  - **Input Port** ingests data from ValidateGooglePlacesData Process Group
  - **AttributesToJSON** generates a JSON representation of the attributes extracted from the FlowFiles and converts XML to JSON format this less attributes.
  - **MergeContent** merges a group of JSON FlowFiles together based on a number of FlowFiles and packages them into a single FlowFile.
  - **UpdateAttribute** updates the attribute name for each FlowFile.
  - **PutFile** writes the contents of the FlowFile to a desired directory on the local filesystem.

## Summary

Congratulations! You learned how to use NextBus's API to connect to their XML Live Feed for vehicle location data. You also learned how to use the **GetHTTP** processor to ingest a live stream from NextBus San Francisco Muni into NiFi!

## Further Reading

- [NextBus XML Live Feed](https://www.nextbus.com/xmlFeedDocs/NextBusXMLFeed.pdf)
- [Hortonworks NiFi User Guide](https://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.0.0/bk_user-guide/content/index.html)
- [Hortonworks NiFi Developer Guide](https://docs.hortonworks.com/HDPDocuments/HDF2/HDF-2.0.0/bk_developer-guide/content/index.html)
